"""
Blog action - Blogging without a language model.

50-star milestone feature. Users write blog news, crawl sites for content,
and SafeClaw generates titles using extractive summarization. The most
summarized/repeated content becomes the blog title. Output is plain .txt.

No GenAI. No LLM. Just summarization algorithms and web crawling.
"""

import logging
import re
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Any
from urllib.parse import urlparse

from safeclaw.actions.base import BaseAction
from safeclaw.core.crawler import Crawler
from safeclaw.core.summarizer import Summarizer

if TYPE_CHECKING:
    from safeclaw.core.engine import SafeClaw

logger = logging.getLogger(__name__)


class BlogAction(BaseAction):
    """
    Blog without a language model.

    Users can:
    - Write blog news entries
    - Crawl websites for content (titles, body, non-title)
    - Auto-generate blog titles from summarized content
    - Output everything as plain .txt

    The title is generated by running extractive summarization on all
    collected entries. The sentence that gets summarized the most -
    the same thing regurgitated - becomes the blog title.
    """

    name = "blog"
    description = "Write blog posts without a language model"

    def __init__(self, blog_dir: Path | None = None):
        self.blog_dir = blog_dir or Path.home() / ".safeclaw" / "blogs"
        self.blog_dir.mkdir(parents=True, exist_ok=True)
        self.summarizer = Summarizer()

    async def execute(
        self,
        params: dict[str, Any],
        user_id: str,
        channel: str,
        engine: "SafeClaw",
    ) -> str:
        """Execute blog action based on natural language input."""
        raw_input = params.get("raw_input", "").strip()
        lower = raw_input.lower()

        # Route based on natural language
        if self._is_help(lower):
            return self._help_text()

        if self._is_crawl_blog(lower):
            return await self._crawl_for_blog(raw_input, user_id, engine)

        if self._is_write(lower):
            return await self._write_blog_news(raw_input, user_id, engine)

        if self._is_show(lower):
            return self._show_blogs(user_id)

        if self._is_generate_title(lower):
            return self._generate_title(user_id)

        if self._is_publish(lower):
            return self._publish_blog(raw_input, user_id)

        # Default: treat as writing blog news
        content = self._extract_blog_content(raw_input)
        if content:
            return await self._write_blog_news(raw_input, user_id, engine)

        return self._help_text()

    # --- Natural language detection ---

    def _is_help(self, text: str) -> bool:
        return bool(re.search(r"blog\s+help|help\s+blog", text))

    def _is_show(self, text: str) -> bool:
        return bool(re.search(
            r"^(show|list|view|read)\s+(my\s+)?blog|^blog\s+(entries|posts|list|show)$",
            text.strip(),
        ))

    def _is_write(self, text: str) -> bool:
        return bool(re.search(
            r"(write|add|post|create)\s+(blog\s+)?(news|entry|post|content)|"
            r"blog\s+(news|write|add|post)",
            text,
        ))

    def _is_crawl_blog(self, text: str) -> bool:
        return bool(re.search(
            r"(crawl|scrape|fetch|grab)\s+.+\s+for\s+.*(title|body|content|heading|text|non.?title)",
            text,
        ))

    def _is_generate_title(self, text: str) -> bool:
        return bool(re.search(
            r"(generate|create|make|suggest)\s+(blog\s+)?(title|headline)|"
            r"blog\s+title",
            text,
        ))

    def _is_publish(self, text: str) -> bool:
        return bool(re.search(
            r"(publish|finalize|save|export)\s+(my\s+)?blog|blog\s+(publish|save|export)",
            text,
        ))

    # --- Blog operations ---

    async def _write_blog_news(
        self, raw_input: str, user_id: str, engine: "SafeClaw"
    ) -> str:
        """Write blog news content to the user's draft."""
        content = self._extract_blog_content(raw_input)
        if not content:
            return "Please provide some content. Example: write blog news The new update brings faster crawling and better summaries."

        draft_path = self._get_draft_path(user_id)

        # Append to existing draft
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
        entry = f"\n[{timestamp}]\n{content}\n"

        with open(draft_path, "a") as f:
            f.write(entry)

        # Count entries
        entry_count = self._count_entries(draft_path)

        # Auto-generate title suggestion from accumulated content
        title_suggestion = ""
        if entry_count >= 2:
            title = self._compute_title(draft_path)
            if title:
                title_suggestion = f"\nSuggested title: {title}"

        return (
            f"Added blog entry ({entry_count} total in draft).\n"
            f"Saved to: {draft_path}"
            f"{title_suggestion}\n\n"
            f"Use 'blog title' to generate a title, 'publish blog' to finalize."
        )

    async def _crawl_for_blog(
        self, raw_input: str, user_id: str, engine: "SafeClaw"
    ) -> str:
        """
        Crawl a website and extract content for the blog.

        Natural language examples:
        - crawl https://example.com for title content
        - crawl example.com for body content
        - crawl example.com for non-title content
        - crawl example.com for heading content
        """
        # Extract URL
        url_match = re.search(
            r'(https?://[^\s]+|(?:www\.)?[a-zA-Z0-9][-a-zA-Z0-9]*\.[a-zA-Z]{2,}[^\s]*)',
            raw_input,
        )
        if not url_match:
            return "Please provide a URL. Example: crawl https://example.com for title content"

        url = url_match.group(1)
        if not url.startswith(("http://", "https://")):
            url = "https://" + url

        # Determine what content to extract
        lower = raw_input.lower()
        extract_type = self._determine_extract_type(lower)

        # Crawl the page
        async with Crawler() as crawler:
            result = await crawler.fetch(url)

        if result.error:
            return f"Could not crawl {url}: {result.error}"

        if not result.text:
            return f"No content found on {url}"

        # Extract the requested content type
        import httpx
        from bs4 import BeautifulSoup

        async with httpx.AsyncClient(
            timeout=30.0,
            headers={"User-Agent": "SafeClaw/0.2 (Privacy-first crawler)"},
        ) as client:
            try:
                response = await client.get(url)
                soup = BeautifulSoup(response.text, "lxml")
            except Exception as e:
                return f"Could not fetch {url}: {e}"

        # Remove script/style
        for element in soup(["script", "style"]):
            element.decompose()

        extracted = self._extract_by_type(soup, extract_type)

        if not extracted:
            return f"No {extract_type} content found on {url}"

        # Write extracted content to blog draft
        draft_path = self._get_draft_path(user_id)
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
        domain = urlparse(url).netloc

        entry = f"\n[{timestamp}] crawled {domain} ({extract_type})\n{extracted}\n"

        with open(draft_path, "a") as f:
            f.write(entry)

        entry_count = self._count_entries(draft_path)

        # Preview
        preview = extracted[:300]
        if len(extracted) > 300:
            preview += "..."

        return (
            f"Crawled {extract_type} content from {domain} and added to blog draft.\n"
            f"Entries in draft: {entry_count}\n\n"
            f"Preview:\n{preview}\n\n"
            f"Use 'blog title' to generate a title from your collected content."
        )

    def _determine_extract_type(self, text: str) -> str:
        """Determine what type of content to extract from natural language."""
        if re.search(r"non.?title|non.?heading|without\s+title", text):
            return "non-title"
        if re.search(r"body|main|article|paragraph|text\s+content", text):
            return "body"
        if re.search(r"title|heading|headline|h[1-6]", text):
            return "title"
        # Default to body
        return "body"

    def _extract_by_type(self, soup: Any, extract_type: str) -> str:
        """Extract content from parsed HTML by type."""
        if extract_type == "title":
            return self._extract_titles(soup)
        elif extract_type == "non-title":
            return self._extract_non_titles(soup)
        else:  # body
            return self._extract_body(soup)

    def _extract_titles(self, soup: Any) -> str:
        """Extract title and heading content from page."""
        titles = []

        # Page title
        title_tag = soup.find("title")
        if title_tag:
            titles.append(title_tag.get_text(strip=True))

        # All headings h1-h6
        for level in range(1, 7):
            for heading in soup.find_all(f"h{level}"):
                text = heading.get_text(strip=True)
                if text and text not in titles:
                    titles.append(text)

        return "\n".join(titles)

    def _extract_non_titles(self, soup: Any) -> str:
        """Extract non-title content (everything except headings)."""
        # Remove headings and title
        for tag in soup.find_all(["title", "h1", "h2", "h3", "h4", "h5", "h6"]):
            tag.decompose()

        # Also remove nav, footer, header for cleaner content
        for tag in soup.find_all(["nav", "footer", "header"]):
            tag.decompose()

        text = soup.get_text(separator="\n", strip=True)
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text

    def _extract_body(self, soup: Any) -> str:
        """Extract main body/article content."""
        # Try to find article or main content first
        main = soup.find("article") or soup.find("main") or soup.find(
            "div", class_=re.compile(r"content|article|post|entry|body", re.I)
        )

        if main:
            # Remove nav/footer/header within
            for tag in main.find_all(["nav", "footer", "header"]):
                tag.decompose()
            text = main.get_text(separator="\n", strip=True)
        else:
            # Fallback: get all paragraph text
            paragraphs = soup.find_all("p")
            text = "\n\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))

        text = re.sub(r'\n{3,}', '\n\n', text)
        return text

    def _generate_title(self, user_id: str) -> str:
        """
        Generate a blog title from accumulated content.

        Uses extractive summarization - the most representative sentence
        from all the collected news/content becomes the title. The content
        that gets summarized the most, the same thing regurgitated, is the title.
        """
        draft_path = self._get_draft_path(user_id)

        if not draft_path.exists():
            return "No blog draft found. Write some entries first with 'write blog news ...'"

        content = draft_path.read_text()
        entries_text = self._get_entries_text(content)

        if not entries_text.strip():
            return "Blog draft is empty. Add some content first."

        # Summarize to a single sentence - the most representative one
        title = self.summarizer.summarize(entries_text, sentences=1)

        if not title or not title.strip():
            # Fallback: use keyword extraction to build a title
            keywords = self.summarizer.get_keywords(entries_text, top_n=5)
            if keywords:
                title = " ".join(w.capitalize() for w in keywords)
            else:
                return "Not enough content to generate a title. Add more entries."

        # Clean up the title
        title = title.strip()
        # Truncate if too long for a title
        if len(title) > 120:
            title = title[:117] + "..."

        return f"Generated blog title:\n\n  {title}\n\nUse 'publish blog' to save with this title, or 'publish blog My Custom Title' to use your own."

    def _publish_blog(self, raw_input: str, user_id: str) -> str:
        """Finalize blog and save as .txt with title."""
        draft_path = self._get_draft_path(user_id)

        if not draft_path.exists():
            return "No blog draft found. Write some entries first."

        content = draft_path.read_text()
        entries_text = self._get_entries_text(content)

        if not entries_text.strip():
            return "Blog draft is empty."

        # Extract custom title or auto-generate
        custom_title = re.sub(
            r'(?i)(publish|finalize|save|export)\s+(my\s+)?blog\s*',
            '',
            raw_input,
        ).strip()

        if custom_title:
            title = custom_title
        else:
            title = self.summarizer.summarize(entries_text, sentences=1).strip()
            if not title:
                keywords = self.summarizer.get_keywords(entries_text, top_n=5)
                title = " ".join(w.capitalize() for w in keywords) if keywords else "Untitled Blog Post"

        # Create the final blog .txt
        timestamp = datetime.now().strftime("%Y-%m-%d")
        safe_title = re.sub(r'[^\w\s-]', '', title)[:50].strip().replace(' ', '-').lower()
        filename = f"{timestamp}-{safe_title}.txt"
        blog_path = self.blog_dir / filename

        blog_content = f"{title}\n{'=' * len(title)}\n\n{entries_text}\n"
        blog_path.write_text(blog_content)

        # Clear the draft
        draft_path.unlink(missing_ok=True)

        return (
            f"Blog published!\n\n"
            f"Title: {title}\n"
            f"Saved to: {blog_path}\n\n"
            f"The blog is a plain .txt file you can share anywhere."
        )

    def _show_blogs(self, user_id: str) -> str:
        """Show existing blog posts and current draft."""
        lines = ["**Your Blog**", ""]

        # Check for draft
        draft_path = self._get_draft_path(user_id)
        if draft_path.exists():
            entry_count = self._count_entries(draft_path)
            lines.append(f"**Draft in progress:** {entry_count} entries")
            lines.append(f"  {draft_path}")
            lines.append("")

        # List published blogs
        blogs = sorted(self.blog_dir.glob("*.txt"))
        # Exclude draft files
        blogs = [b for b in blogs if not b.name.startswith("draft-")]

        if blogs:
            lines.append(f"**Published blogs:** ({len(blogs)} total)")
            for blog in blogs[-10:]:
                # Read first line as title
                first_line = blog.read_text().split("\n", 1)[0]
                lines.append(f"  - {first_line} ({blog.name})")
        else:
            lines.append("No published blogs yet.")

        lines.extend([
            "",
            "Commands: 'write blog news ...', 'crawl <url> for title content',",
            "'blog title', 'publish blog'",
        ])

        return "\n".join(lines)

    # --- Helpers ---

    def _get_draft_path(self, user_id: str) -> Path:
        """Get the draft file path for a user."""
        safe_id = re.sub(r'[^\w]', '_', user_id)
        return self.blog_dir / f"draft-{safe_id}.txt"

    def _count_entries(self, draft_path: Path) -> int:
        """Count entries in a draft file."""
        if not draft_path.exists():
            return 0
        content = draft_path.read_text()
        return len(re.findall(r'\[\d{4}-\d{2}-\d{2} \d{2}:\d{2}\]', content))

    def _get_entries_text(self, content: str) -> str:
        """Extract just the text content from entries, stripping timestamps."""
        lines = []
        for line in content.split("\n"):
            # Skip timestamp lines
            if re.match(r'\[\d{4}-\d{2}-\d{2} \d{2}:\d{2}\]', line.strip()):
                continue
            if line.strip():
                lines.append(line.strip())
        return "\n".join(lines)

    def _extract_blog_content(self, raw_input: str) -> str:
        """Extract blog content from natural language input."""
        # Remove the command prefix
        content = re.sub(
            r'(?i)^(write|add|post|create)\s+(blog\s+)?(news|entry|post|content)\s*',
            '',
            raw_input,
        )
        content = re.sub(
            r'(?i)^blog\s+(news|write|add|post)\s*',
            '',
            content,
        )
        return content.strip()

    def _compute_title(self, draft_path: Path) -> str:
        """Compute a title from draft content."""
        content = draft_path.read_text()
        entries_text = self._get_entries_text(content)
        if not entries_text.strip():
            return ""
        title = self.summarizer.summarize(entries_text, sentences=1).strip()
        if len(title) > 120:
            title = title[:117] + "..."
        return title

    def _help_text(self) -> str:
        """Return blog help text."""
        return (
            "**Blog - Write without a language model**\n"
            "\n"
            "50-star milestone feature. Write blog news, crawl sites\n"
            "for content, and SafeClaw generates your titles using\n"
            "extractive summarization. No AI. Just math.\n"
            "\n"
            "**Write blog entries:**\n"
            "  write blog news The latest release improves crawling speed.\n"
            "  blog news tech We added RSS feed support for 50 new sources.\n"
            "  add blog entry Privacy-first tools are gaining traction.\n"
            "\n"
            "**Crawl sites for content:**\n"
            "  crawl https://example.com for title content\n"
            "  crawl https://example.com for body content\n"
            "  crawl https://example.com for non-title content\n"
            "\n"
            "**Generate title and publish:**\n"
            "  blog title        - Generate a title from your entries\n"
            "  publish blog      - Save as .txt with auto-generated title\n"
            "  publish blog My Custom Title  - Save with your own title\n"
            "\n"
            "**View your blog:**\n"
            "  show blog          - See draft and published posts\n"
            "\n"
            "The title comes from summarization: the most repeated,\n"
            "most representative content becomes your headline."
        )
